{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12e91914-5f51-43fa-b65b-625e73b4d17b",
   "metadata": {
    "id": "12e91914-5f51-43fa-b65b-625e73b4d17b"
   },
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp?1\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2520ec3-722f-4f44-bdd1-885b13e7afbf",
   "metadata": {
    "id": "c2520ec3-722f-4f44-bdd1-885b13e7afbf"
   },
   "source": [
    "# Chapter 7: Finetuning To Follow Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e19327b-6c02-4881-ad02-9b6d3ec0b1b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4e19327b-6c02-4881-ad02-9b6d3ec0b1b4",
    "outputId": "9d937b84-d8f8-4ce9-cc3c-211188f49a10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.9.2\n",
      "tiktoken version: 0.8.0\n",
      "torch version: 2.5.1\n",
      "tqdm version: 4.67.1\n",
      "tensorflow version: 2.17.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"matplotlib\",  # Plotting library\n",
    "    \"tiktoken\",    # Tokenizer\n",
    "    \"torch\",       # Deep learning library\n",
    "    \"tqdm\",        # Progress bar\n",
    "    \"tensorflow\",  # For OpenAI's pretrained weights\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264fca98-2f9a-4193-b435-2abfa3b4142f",
   "metadata": {
    "id": "264fca98-2f9a-4193-b435-2abfa3b4142f"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/overview.webp?1\" width=1000px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbc68e9-75b3-41f1-ac2c-e071c3cd0813",
   "metadata": {
    "id": "8bbc68e9-75b3-41f1-ac2c-e071c3cd0813"
   },
   "source": [
    "## 7.1 Introduction to instruction finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dba24a-6805-496c-9a7f-c75e2d3527ab",
   "metadata": {
    "id": "53dba24a-6805-496c-9a7f-c75e2d3527ab"
   },
   "source": [
    "- 在第五章中，我们了解到对大语言模型（LLM）进行预训练涉及一个训练过程，在这个过程中它学习一次生成一个单词。\n",
    "- 因此，一个经过预训练的大语言模型擅长文本补全，但不擅长遵循指令。\n",
    "- 在本章中，我们将教导大语言模型更好地遵循指令。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dc0535-0904-44ed-beaf-9b678292ef35",
   "metadata": {
    "id": "18dc0535-0904-44ed-beaf-9b678292ef35"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/instruction-following.webp\" width=800px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4698b23-12e0-4bd7-a140-ccb3dd71d4e8",
   "metadata": {
    "id": "b4698b23-12e0-4bd7-a140-ccb3dd71d4e8"
   },
   "source": [
    "- 本章所涵盖的主题总结在下面的图表中。 \n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/chapter-overview-1.webp?1\" width=800px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5384f0cf-ef3c-4436-a5fa-59bd25649f86",
   "metadata": {
    "id": "5384f0cf-ef3c-4436-a5fa-59bd25649f86"
   },
   "source": [
    "## 7.2 Preparing a dataset for supervised instruction finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b34ff8-619f-4e89-bd03-ce513269760d",
   "metadata": {
    "id": "f8b34ff8-619f-4e89-bd03-ce513269760d"
   },
   "source": [
    "- 我们将使用我为本章准备的一个指令数据集来开展工作。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0G3axLw6kY1N",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0G3axLw6kY1N",
    "outputId": "a5f70eb8-6248-4834-e7ae-6105e94e5afa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "\n",
    "def download_and_load_file(file_path, url):\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode(\"utf-8\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "    else:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text_data = file.read()\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "file_path = \"instruction-data.json\"\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
    "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    ")\n",
    "\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7af8176-4255-4e92-8c7d-998771733eb8",
   "metadata": {
    "id": "d7af8176-4255-4e92-8c7d-998771733eb8"
   },
   "source": [
    "- 我们从上述 JSON 文件中加载的 `data` 列表中的每一项 都是一个具有以下形式的字典。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "-LiuBMsHkzQV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-LiuBMsHkzQV",
    "outputId": "cc742019-b8d7-40f9-b21a-6a5ddf821377"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example entry:\n",
      " {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}\n"
     ]
    }
   ],
   "source": [
    "print(\"Example entry:\\n\", data[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a32b34-485a-4816-a77a-da14f9fe6e46",
   "metadata": {
    "id": "c5a32b34-485a-4816-a77a-da14f9fe6e46"
   },
   "source": [
    "- Note that the `'input'` field can be empty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "uFInFxDDk2Je",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uFInFxDDk2Je",
    "outputId": "70241295-a9ec-4b7d-caf5-ab6f267e3271"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another example entry:\n",
      " {'instruction': \"What is an antonym of 'complicated'?\", 'input': '', 'output': \"An antonym of 'complicated' is 'simple'.\"}\n"
     ]
    }
   ],
   "source": [
    "print(\"Another example entry:\\n\", data[999])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f034799a-6575-45fd-98c9-9d1012d0fd58",
   "metadata": {
    "id": "f034799a-6575-45fd-98c9-9d1012d0fd58"
   },
   "source": [
    "- 指令微调通常被称为“有监督的指令微调 supervised instruction finetuning”，因为它涉及在一个明确提供了输入-输出对的数据集上对模型进行训练。\n",
    "- 有不同的方式来格式化条目，使其作为大语言模型（LLM）的输入；下面的图表分别展示了两种示例格式，它们曾被用于 Alpaca 大语言模型（https://crfm.stanford.edu/2023/03/13/alpaca.html）和 Phi-3 大语言模型（https://arxiv.org/abs/2404.14219）。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffa4f70-44d4-4be4-89a9-2159f4885b10",
   "metadata": {
    "id": "dffa4f70-44d4-4be4-89a9-2159f4885b10"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/prompt-style.webp?1\" width=1000px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd79a74e-befb-491c-be49-f777a6a5b6a6",
   "metadata": {
    "id": "dd79a74e-befb-491c-be49-f777a6a5b6a6"
   },
   "source": [
    "- 在本章中，我们使用 Alpaca 风格的提示格式，这是用于指令微调的原始提示模板。\n",
    "- 下面，我们对将作为输入传递给大语言模型（LLM）的内容进行格式化。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "Jhk37nnJnkBh",
   "metadata": {
    "id": "Jhk37nnJnkBh"
   },
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011e78b4-e89a-4653-a2ee-7b2739ca04d6",
   "metadata": {
    "id": "011e78b4-e89a-4653-a2ee-7b2739ca04d6"
   },
   "source": [
    "- 带有输入字段的格式化回复如下所示。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "F9UQRfjzo4Js",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F9UQRfjzo4Js",
    "outputId": "13ec7abf-ad94-4e26-860d-6a39a344f31f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the correct spelling of the following word.\n",
      "\n",
      "### Input:\n",
      "Ocassion\n",
      "\n",
      "### Response:\n",
      "The correct spelling is 'Occasion.'\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[50])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
    "\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc93ddf-431c-49c0-96f2-fb3a79c4d94c",
   "metadata": {
    "id": "4dc93ddf-431c-49c0-96f2-fb3a79c4d94c"
   },
   "source": [
    "- 下面是一个没有输入字段的格式化回复。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3891fa9-f738-41cd-946c-80ef9a99c346",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3891fa9-f738-41cd-946c-80ef9a99c346",
    "outputId": "d6be5713-1293-4a70-c8c8-a86ea8e95817"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is an antonym of 'complicated'?\n",
      "\n",
      "### Response:\n",
      "An antonym of 'complicated' is 'simple'.\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[999])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[999]['output']}\"\n",
    "\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa8afd5-2a21-49a5-90c3-6a03865a4771",
   "metadata": {
    "id": "4aa8afd5-2a21-49a5-90c3-6a03865a4771"
   },
   "source": [
    "- 最后，在下一节准备 PyTorch dataloader 之前，我们将数据集划分为训练集、验证集和测试集。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aFZVopbIlNfx",
   "metadata": {
    "id": "aFZVopbIlNfx"
   },
   "outputs": [],
   "source": [
    "train_portion = int(len(data) * 0.85)  # 85% for training\n",
    "test_portion = int(len(data) * 0.1)    # 10% for testing\n",
    "val_portion = len(data) - train_portion - test_portion  # Remaining 5% for validation\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "-zf6oht6bIUQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-zf6oht6bIUQ",
    "outputId": "bb5fe8e5-1ce5-4fca-a430-76ecf42e99ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 935\n",
      "Validation set length: 55\n",
      "Test set length: 110\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaaf606-f913-4445-8301-632ae10d387d",
   "metadata": {
    "id": "fcaaf606-f913-4445-8301-632ae10d387d"
   },
   "source": [
    "## 7.3 Organizing data into training batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233f63bd-9755-4d07-8884-5e2e5345cf27",
   "metadata": {
    "id": "233f63bd-9755-4d07-8884-5e2e5345cf27"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/chapter-overview-2.webp?1\" width=800px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c149fc1a-7757-4ec8-80cb-e2a3fb007a2c",
   "metadata": {
    "id": "c149fc1a-7757-4ec8-80cb-e2a3fb007a2c"
   },
   "source": [
    "- 我们分几个步骤来处理这个数据集批处理（dataset batching），如下图所示进行了总结。 \n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/detailed-batching.webp?1\" width=800px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9af423f-aad9-4b3c-bea5-153021c04862",
   "metadata": {
    "id": "b9af423f-aad9-4b3c-bea5-153021c04862"
   },
   "source": [
    "- 首先，我们实现了一个“指令数据集（InstructionDataset）”类，它会对数据集中的所有输入进行预分词（pre-tokenizes）处理，这与第6章中的“垃圾邮件数据集（SpamDataset）”类似。 \n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/pretokenizing.webp\" width=1000px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adc29dc4-f1c7-4c71-937b-95119d6239bb",
   "metadata": {
    "id": "adc29dc4-f1c7-4c71-937b-95119d6239bb"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "\n",
    "        # Pre-tokenize texts\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384f0e69-4b22-41c0-a25d-f077527eddd1",
   "metadata": {
    "id": "384f0e69-4b22-41c0-a25d-f077527eddd1"
   },
   "source": [
    "- 与第六章类似，我们希望在一个批次中收集多个训练样本，以加速训练过程；这就需要将所有输入都填充到相近的长度。\n",
    "- 同样与上一章类似，我们使用 `<|endoftext|>` 标记作为填充标记。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff24fe1a-5746-461c-ad3d-b6d84a1a7c96",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ff24fe1a-5746-461c-ad3d-b6d84a1a7c96",
    "outputId": "4d63f8b8-b4ad-45d9-9e93-c9dd8c2b7706"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5bd7bc-f347-4cf8-a0c2-94cb8799e427",
   "metadata": {
    "id": "9e5bd7bc-f347-4cf8-a0c2-94cb8799e427"
   },
   "source": [
    "- 在第六章中，我们将数据集中的所有样本都填充到了相同的长度。\n",
    "  - 在这里，我们采用一种更为复杂精细的方法，开发一个自定义的 “collate” 函数，并将其传递给数据加载器。\n",
    "  - 这个自定义的 collate 函数会将每个批次中的训练样本填充到相同的长度（不过不同的批次可以有不同的长度）。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c4d943-4aa8-4a44-874e-05bc6831fbd3",
   "metadata": {
    "id": "65c4d943-4aa8-4a44-874e-05bc6831fbd3"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/padding.webp\" width=800px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb4c77dd-c956-4a1b-897b-b466909f18ca",
   "metadata": {
    "id": "eb4c77dd-c956-4a1b-897b-b466909f18ca"
   },
   "outputs": [],
   "source": [
    "def custom_collate_draft_1(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    # and increase the max length by +1, which will add one extra\n",
    "    # padding token below\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # Pad and prepare inputs\n",
    "    inputs_lst = []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to batch_max_length\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        # Via padded[:-1], we remove the extra padded token\n",
    "        # that has been added via the +1 setting in batch_max_length\n",
    "        # (the extra padding token will be relevant in later codes)\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        inputs_lst.append(inputs)\n",
    "\n",
    "    # Convert list of inputs to tensor and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    return inputs_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8fb02373-59b3-4f3a-b1d1-8181a2432645",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8fb02373-59b3-4f3a-b1d1-8181a2432645",
    "outputId": "8705ca9a-e999-4f70-9db8-1ad084eba7bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "\n",
    "batch = (\n",
    "    inputs_1,\n",
    "    inputs_2,\n",
    "    inputs_3\n",
    ")\n",
    "\n",
    "print(custom_collate_draft_1(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46832ab-39b7-45f8-b330-ac9adfa10d1b",
   "metadata": {
    "id": "c46832ab-39b7-45f8-b330-ac9adfa10d1b"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/batching-step-4.webp?1\" width=1000px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17769a19-b961-4213-92ef-34f441b2d1d6",
   "metadata": {
    "id": "17769a19-b961-4213-92ef-34f441b2d1d6"
   },
   "source": [
    "- 上述内容中，我们只向大语言模型（LLM）输入了数据；然而，对于大语言模型的训练而言，我们还需要目标值。\n",
    "- 与预训练大语言模型类似，目标值是将输入内容向右移动一个位置得到的，这样大语言模型就能学习预测下一个标记。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0386b6fe-3455-4e70-becd-a5a4681ba2ef",
   "metadata": {
    "id": "0386b6fe-3455-4e70-becd-a5a4681ba2ef"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/inputs-targets.webp?1\" width=800px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74af192e-757c-4c0a-bdf9-b7eb25bf6ebc",
   "metadata": {
    "id": "74af192e-757c-4c0a-bdf9-b7eb25bf6ebc"
   },
   "outputs": [],
   "source": [
    "def custom_collate_draft_2(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # Pad and prepare inputs\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to max_length\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
    "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    # Convert list of inputs to tensor and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6eb2bce3-28a7-4f39-9d4b-5e972d69066c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6eb2bce3-28a7-4f39-9d4b-5e972d69066c",
    "outputId": "b9ceae14-13c2-49f7-f4a4-b503f3db3009"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256, 50256, 50256, 50256],\n",
      "        [    8,     9, 50256, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "inputs, targets = custom_collate_draft_2(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf85703-a0e0-42aa-8f29-cbc28dbf4e15",
   "metadata": {
    "id": "3bf85703-a0e0-42aa-8f29-cbc28dbf4e15"
   },
   "source": [
    "- 接下来，我们引入一个`ignore_index`值，用一个新的值来替换所有的填充（padding）标记 ID；这个`ignore_index`的目的是我们可以在损失函数中忽略填充值（关于这一点，稍后会详细介绍）。\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/batching-step-5.webp?1\" width=800px>\n",
    "\n",
    "- 具体来说，这意味着我们将与`50256`对应的标记 ID 替换为`-100`，如下所示。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4bed33-956e-4b3f-a09c-586d8203109a",
   "metadata": {
    "id": "bd4bed33-956e-4b3f-a09c-586d8203109a"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/ignore-index.webp?1\" width=800px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5346513e-c3f4-44fe-af22-4ebd36497728",
   "metadata": {
    "id": "5346513e-c3f4-44fe-af22-4ebd36497728"
   },
   "source": [
    "- （此外，我们还引入了`allowed_max_length`，以防我们想要限制样本的长度；如果你计划处理比 GPT-2 模型所支持的 1024 个标记的上下文长度还要长的自有数据集，这将会很有用。） "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41ec6e2d-9eb2-4124-913e-d2af39be4cf2",
   "metadata": {
    "id": "41ec6e2d-9eb2-4124-913e-d2af39be4cf2"
   },
   "outputs": [],
   "source": [
    "def custom_collate_fn(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    ignore_index=-100,\n",
    "    allowed_max_length=None,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # Pad and prepare inputs and targets\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to max_length\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
    "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
    "\n",
    "        # New: Replace all but the first padding tokens in targets by ignore_index\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "\n",
    "        # New: Optionally truncate to maximum sequence length\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    # Convert list of inputs and targets to tensors and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cdf5eec4-9ebe-4be0-9fca-9a47bee88fdc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cdf5eec4-9ebe-4be0-9fca-9a47bee88fdc",
    "outputId": "a5501547-239d-431d-fb04-da7fa2ffad79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256,  -100,  -100,  -100],\n",
      "        [    8,     9, 50256,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "inputs, targets = custom_collate_fn(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26727c90-0d42-43b3-af21-0a66ad4fbbc7",
   "metadata": {
    "id": "26727c90-0d42-43b3-af21-0a66ad4fbbc7"
   },
   "source": [
    "- 让我们看看用 -100 进行的这种替换能达成什么效果。\n",
    "- 为了便于说明，我们假设我们有一个小型的分类任务，有两个类别标签，即 0 和 1，这与第 6 章的情况类似。\n",
    "- 如果我们有以下的对数几率（logits）值（模型最后一层的输出），我们计算出以下的损失。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "W2jvh-OP9MFV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W2jvh-OP9MFV",
    "outputId": "b5cd858e-7c58-4a21-c5a7-e72768bd301c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1269)\n"
     ]
    }
   ],
   "source": [
    "logits_1 = torch.tensor(\n",
    "    [[-1.0, 1.0],  # 1st training example\n",
    "     [-0.5, 1.5]]  # 2nd training example\n",
    ")\n",
    "targets_1 = torch.tensor([0, 1])\n",
    "\n",
    "\n",
    "loss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)\n",
    "print(loss_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edd3244-8886-4505-92e9-367d28529e1e",
   "metadata": {
    "id": "5edd3244-8886-4505-92e9-367d28529e1e"
   },
   "source": [
    "- 现在，不出所料，再添加一个训练示例将会影响损失值。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "nvVMuil89v9N",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nvVMuil89v9N",
    "outputId": "e4a07b99-a23c-4404-ccdb-5f93c39f3b09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7936)\n"
     ]
    }
   ],
   "source": [
    "logits_2 = torch.tensor(\n",
    "    [[-1.0, 1.0],\n",
    "     [-0.5, 1.5],\n",
    "     [-0.5, 1.5]]  # New 3rd training example\n",
    ")\n",
    "targets_2 = torch.tensor([0, 1, 1])\n",
    "\n",
    "loss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)\n",
    "print(loss_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dca331-40e0-468b-b690-189fe156ba8f",
   "metadata": {
    "id": "54dca331-40e0-468b-b690-189fe156ba8f"
   },
   "source": [
    "- 让我们看看如果我们将其中一个示例的类别标签替换为 -100 会发生什么情况。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "RTyB1vah9p56",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RTyB1vah9p56",
    "outputId": "28c16387-1d9c-48a7-eda7-aa270864683d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1269)\n",
      "loss_1 == loss_3: tensor(True)\n"
     ]
    }
   ],
   "source": [
    "targets_3 = torch.tensor([0, 1, -100])\n",
    "\n",
    "loss_3 = torch.nn.functional.cross_entropy(logits_2, targets_3)\n",
    "print(loss_3)\n",
    "print(\"loss_1 == loss_3:\", loss_1 == loss_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef09d21-b652-4760-abea-4f76920e6a25",
   "metadata": {
    "id": "cef09d21-b652-4760-abea-4f76920e6a25"
   },
   "source": [
    "- 正如我们所见，这 3 个训练示例的最终损失与我们从 2 个训练示例中计算出的损失相同，这意味着交叉熵损失函数忽略了标签为 -100 的那个训练示例。\n",
    "- 默认情况下，PyTorch 有 `cross_entropy(..., ignore_index=-100)` 这样的设置，用于忽略与标签 -100 相对应的示例。\n",
    "- 通过使用这个 -100 的 `ignore_index`，我们可以忽略批次中额外的文本结束（填充）标记，这些标记是我们用来将训练示例填充到等长时所使用的。\n",
    "- 然而，我们并不想忽略第一个文本结束（填充）标记（50256）的实例，因为当大语言模型（LLM）的回复完成时，它可以起到提示作用。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4e9c5f-7c49-4321-9f1b-a50468a84524",
   "metadata": {
    "id": "6a4e9c5f-7c49-4321-9f1b-a50468a84524"
   },
   "source": [
    "- In practice, it is also common to mask out the target token IDs that correspond to the instruction, as illustrated in the figure below (this is a recommended reader exercise after completing the chapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab8f0ed-80e8-4fd9-bf84-e5d0e0bc0a39",
   "metadata": {
    "id": "fab8f0ed-80e8-4fd9-bf84-e5d0e0bc0a39"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/mask-instructions.webp?1\" width=1000px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccaf048-ec95-498c-9155-d5b3ccba6c96",
   "metadata": {
    "id": "bccaf048-ec95-498c-9155-d5b3ccba6c96"
   },
   "source": [
    "## 7.4 为指令数据集创建 data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b8e656-3af3-4db6-8dde-d8c216a12f50",
   "metadata": {
    "id": "e6b8e656-3af3-4db6-8dde-d8c216a12f50"
   },
   "source": [
    "- 在本节中，我们使用 `InstructionDataset` 类和 `custom_collate_fn` 函数来实例化训练、验证和测试数据加载器。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fffe390-b226-4d5c-983f-9f4da773cb82",
   "metadata": {
    "id": "9fffe390-b226-4d5c-983f-9f4da773cb82"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/chapter-overview-3.webp?1\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932677e9-9317-42e8-b461-7b0269518f97",
   "metadata": {
    "id": "932677e9-9317-42e8-b461-7b0269518f97"
   },
   "source": [
    "- 之前的 `custom_collate_fn` 函数还有一个额外的细节，即我们现在直接将数据移动到目标设备（例如 GPU），而不是在主训练循环中进行这一操作。这样做可以提高效率，因为当我们将 `custom_collate_fn` 作为数据加载器的一部分使用时，该操作可以作为后台进程执行。\n",
    "- 我们使用 Python 标准库 `functools` 中的 `partial` 函数，创建了一个新函数，该函数预先填充了原函数的 `device` 参数。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "etpqqWh8phKc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "etpqqWh8phKc",
    "outputId": "925faf3a-6df4-4ad0-f276-f328493619c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Note:\n",
    "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
    "# which is much faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
    "# However, the resulting loss values may be slightly different.\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "   device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "   device = torch.device(\"mps\")\n",
    "else:\n",
    "   device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e47fb30-c2c6-4e6d-a64c-76cc65be4a2c",
   "metadata": {
    "id": "4e47fb30-c2c6-4e6d-a64c-76cc65be4a2c"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "customized_collate_fn = partial(\n",
    "    custom_collate_fn,\n",
    "    device=device,\n",
    "    allowed_max_length=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff42c29-8b81-45e5-ae8d-b97cd1cf447a",
   "metadata": {
    "id": "8ff42c29-8b81-45e5-ae8d-b97cd1cf447a"
   },
   "source": [
    "- 接下来，我们像前几章那样实例化数据加载器，不同的是，我们现在为批处理过程提供了自己的整理（collate）函数。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "BtWkgir6Hlpe",
   "metadata": {
    "id": "BtWkgir6Hlpe"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d097dc8-ad34-4f05-b435-e4147965f532",
   "metadata": {
    "id": "1d097dc8-ad34-4f05-b435-e4147965f532"
   },
   "outputs": [],
   "source": [
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f67c147-b1a2-4a95-9807-e2d0de0324c0",
   "metadata": {
    "id": "3f67c147-b1a2-4a95-9807-e2d0de0324c0"
   },
   "source": [
    "- 让我们看看最终得到的输入批次和目标批次的维度是什么样的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "GGs1AI3vHpnX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GGs1AI3vHpnX",
    "outputId": "53a9695d-87cb-4d7c-8b43-1561dfa68ba0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 89]) torch.Size([8, 89])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 88]) torch.Size([8, 88])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 58]) torch.Size([8, 58])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 87]) torch.Size([8, 87])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 57]) torch.Size([8, 57])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 82]) torch.Size([8, 82])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for inputs, targets in train_loader:\n",
    "    print(inputs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8e8dd7-d46a-4cc3-8a7e-c1d31e1b4657",
   "metadata": {
    "id": "0c8e8dd7-d46a-4cc3-8a7e-c1d31e1b4657"
   },
   "source": [
    "- 从上面的输出我们可以看到，正如预期的那样，所有批次的批量大小均为 8，但长度不同。\n",
    "- 让我们再通过打印 `inputs` 批次中第一个训练示例的内容，来再次确认输入是否包含与标记 ID 50256 对应的 `<|endoftext|>` 填充标记。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "21b8fd02-014f-4481-9b71-5bfee8f9dfcd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21b8fd02-014f-4481-9b71-5bfee8f9dfcd",
    "outputId": "ce919ecd-5ded-453c-a312-10cf55c13da7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n",
      "          257,  2882,   326, 20431, 32543,   262,  2581,    13,   198,   198,\n",
      "        21017, 46486,    25,   198, 30003,  6525,   262,  6827,  1262,   257,\n",
      "          985,   576,    13,   198,   198, 21017, 23412,    25,   198,   464,\n",
      "         5156,   318,   845, 13779,    13,   198,   198, 21017, 18261,    25,\n",
      "          198,   464,  5156,   318,   355, 13779,   355,   257,  4936,    13,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(inputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1f3647-8971-4006-89e0-6a2a1ec1d360",
   "metadata": {
    "id": "5f1f3647-8971-4006-89e0-6a2a1ec1d360"
   },
   "source": [
    "- 同样地，我们通过肉眼检查来再次确认目标数据中包含 -100 占位标记。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "51649ab4-1a7e-4a9e-92c5-950a24fde211",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51649ab4-1a7e-4a9e-92c5-950a24fde211",
    "outputId": "fdf486f3-e99d-4891-9814-afc9e4991020"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,   257,\n",
      "         2882,   326, 20431, 32543,   262,  2581,    13,   198,   198, 21017,\n",
      "        46486,    25,   198, 30003,  6525,   262,  6827,  1262,   257,   985,\n",
      "          576,    13,   198,   198, 21017, 23412,    25,   198,   464,  5156,\n",
      "          318,   845, 13779,    13,   198,   198, 21017, 18261,    25,   198,\n",
      "          464,  5156,   318,   355, 13779,   355,   257,  4936,    13, 50256,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(targets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6aad445-8f19-4238-b9bf-db80767fb91a",
   "metadata": {
    "id": "d6aad445-8f19-4238-b9bf-db80767fb91a"
   },
   "source": [
    "## 7.5 加载预训练 LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5c07d1-4fc9-4846-94cf-b11a085a667b",
   "metadata": {
    "id": "5a5c07d1-4fc9-4846-94cf-b11a085a667b"
   },
   "source": [
    "- 在本节中，我们使用与第 5 章第 5.5 节以及第 6 章第 6.4 节相同的代码来加载一个预训练的 GPT 模型。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b438f-88af-413f-96a9-f059c6c55fc4",
   "metadata": {
    "id": "8d1b438f-88af-413f-96a9-f059c6c55fc4"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/chapter-overview-4.webp?1\" width=800px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c68eda7-e02e-4caa-846b-ca6dbd396ca2",
   "metadata": {
    "id": "8c68eda7-e02e-4caa-846b-ca6dbd396ca2"
   },
   "source": [
    "- 然而，我们没有加载参数数量最少的 1.24 亿参数模型，而是加载了拥有 3.55 亿参数的中等版本模型，因为 1.24 亿参数的模型太小，通过指令微调难以取得质量上合理的结果。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0d249d67-5eba-414e-9bd2-972ebf01329d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0d249d67-5eba-414e-9bd2-972ebf01329d",
    "outputId": "3f08f5e1-ca7c-406d-e2ae-1b5fcafad3f2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 24.1kiB/s]\n",
      "encoder.json: 100%|██████████| 1.04M/1.04M [00:01<00:00, 871kiB/s] \n",
      "hparams.json: 100%|██████████| 91.0/91.0 [00:00<00:00, 21.4kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|██████████| 1.42G/1.42G [09:45<00:00, 2.42MiB/s] \n",
      "model.ckpt.index: 100%|██████████| 10.4k/10.4k [00:00<00:00, 2.10MiB/s]\n",
      "model.ckpt.meta: 100%|██████████| 927k/927k [00:01<00:00, 648kiB/s]  \n",
      "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 470kiB/s] \n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2_local\n",
    "from previous_chapters import GPTModel, load_weights_into_gpt\n",
    "\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2_local(\n",
    "    model_size=model_size,\n",
    "    models_dir=\"gpt2\",\n",
    "    local_only=False\n",
    "\n",
    ")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf3afed-bc8e-4d3a-ad9d-eb6f57bb7af5",
   "metadata": {
    "id": "dbf3afed-bc8e-4d3a-ad9d-eb6f57bb7af5"
   },
   "source": [
    "- 在我们在下一节开始对模型进行微调之前，让我们看看它在其中一个验证任务上的表现如何。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7bd32b7c-5b44-4d25-a09f-46836802ca74",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7bd32b7c-5b44-4d25-a09f-46836802ca74",
    "outputId": "30d4fbd9-7d22-4545-cfc5-c5749cc0bd93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "input_text = format_input(val_data[0])\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2e3e68e0-2627-4c65-b4e7-1e0667e4f6fa",
   "metadata": {
    "id": "2e3e68e0-2627-4c65-b4e7-1e0667e4f6fa"
   },
   "outputs": [],
   "source": [
    "from previous_chapters import (\n",
    "    generate,\n",
    "    text_to_token_ids,\n",
    "    token_ids_to_text\n",
    ")\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(input_text, tokenizer),\n",
    "    max_new_tokens=35,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    eos_id=50256,\n",
    ")\n",
    "generated_text = token_ids_to_text(token_ids, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e2fda5-f796-4954-8f72-1dd1123e3344",
   "metadata": {
    "id": "36e2fda5-f796-4954-8f72-1dd1123e3344"
   },
   "source": [
    "- 请注意，我们在前面几章中使用的 `generate` 函数会返回组合后的输入和输出文本，这在前面的章节中便于生成易读的文本。\n",
    "- 为了分离出回复内容，我们可以从 `generated_text` 的起始位置减去指令的长度。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ba4a55bf-a245-48d8-beda-2838a58fb5ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ba4a55bf-a245-48d8-beda-2838a58fb5ba",
    "outputId": "b46de9b3-98f0-45e4-a9ae-86870c3244a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chef cooks the meal every day.\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "Convert the active sentence to passive: 'The chef cooks the\n"
     ]
    }
   ],
   "source": [
    "response_text = (\n",
    "    generated_text[len(input_text):]\n",
    "    .replace(\"### Response:\", \"\")\n",
    "    .strip()\n",
    ")\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44080b2-a4c5-4520-a797-549519f66a3e",
   "metadata": {
    "id": "d44080b2-a4c5-4520-a797-549519f66a3e"
   },
   "source": [
    "- 正如我们所见，该模型目前还无法遵循指令；它创建了一个“回复”部分，但只是重复了原始输入句子以及指令内容。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d27b9d-a942-4cf5-b797-848c5f01e723",
   "metadata": {
    "id": "70d27b9d-a942-4cf5-b797-848c5f01e723"
   },
   "source": [
    "## 7.6 在指令数据上微调大语言模型（LLM）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314b2a39-88b4-44d8-8c85-1c5b0cd6cc4a",
   "metadata": {
    "id": "314b2a39-88b4-44d8-8c85-1c5b0cd6cc4a"
   },
   "source": [
    "- 在本节中，我们将对模型进行微调。\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/chapter-overview-5.webp?1\" width=800px>\n",
    "\n",
    "- 请注意，我们可以复用在前面几章中使用过的所有损失计算和训练函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "65444865-df87-4d98-9faf-875e1c4be860",
   "metadata": {
    "id": "65444865-df87-4d98-9faf-875e1c4be860"
   },
   "outputs": [],
   "source": [
    "from previous_chapters import (\n",
    "    calc_loss_loader,\n",
    "    train_model_simple\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00083059-aa41-4d37-8a17-1c72d1b1ca00",
   "metadata": {
    "id": "00083059-aa41-4d37-8a17-1c72d1b1ca00"
   },
   "source": [
    "- 让我们在开始训练之前计算初始的训练集和验证集损失（和前面几章一样，我们的目标是最小化损失）。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d99fc6f8-63b2-43da-adbb-a7b6b92c8dd5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d99fc6f8-63b2-43da-adbb-a7b6b92c8dd5",
    "outputId": "36fdf03b-6fa6-46c3-c77d-ecc99e886265"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.8259105682373047\n",
      "Validation loss: 3.7619349479675295\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a6da8f-15b3-42b0-a136-619b7a35c3e9",
   "metadata": {
    "id": "12a6da8f-15b3-42b0-a136-619b7a35c3e9"
   },
   "source": [
    "- 请注意，由于我们使用了更大的模型（3.55 亿参数而非 1.24 亿参数），本次训练的成本比前几章的训练要高一些。\n",
    "- 以下列出了在不同设备上的运行时间，以供参考（在兼容的 GPU 设备上运行此笔记本无需对代码进行修改）。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4b57fb-e689-4550-931c-6d34a932487c",
   "metadata": {
    "id": "db4b57fb-e689-4550-931c-6d34a932487c"
   },
   "source": [
    "<div style=\"text-align: left;\">\n",
    "    \n",
    "| Model              | Device                | Runtime for 2 Epochs |\n",
    "|--------------------|-----------------------|----------------------|\n",
    "| gpt2-medium (355M) | CPU (M3 MacBook Air)  | 15.78 minutes        |\n",
    "| gpt2-medium (355M) | GPU (M3 MacBook Air)  | 10.77 minutes        |\n",
    "| gpt2-medium (355M) | GPU (L4)              | 1.83 minutes         |\n",
    "| gpt2-medium (355M) | GPU (A100)            | 0.86 minutes         |\n",
    "| gpt2-small (124M)  | CPU (M3 MacBook Air)  | 5.74 minutes         |\n",
    "| gpt2-small (124M)  | GPU (M3 MacBook Air)  | 3.73 minutes         |\n",
    "| gpt2-small (124M)  | GPU (L4)              | 0.69 minutes         |\n",
    "| gpt2-small (124M)  | GPU (A100)            | 0.39 minutes         |\n",
    "\n",
    "</div>\n",
    "\n",
    "- I ran this notebook using the `\"gpt2-medium (355M)\"` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "78bcf83a-1fff-4540-97c1-765c4016d5e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "78bcf83a-1fff-4540-97c1-765c4016d5e3",
    "outputId": "cea0618c-56ca-418a-c972-bcc060362727"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 2.637, Val loss 2.626\n",
      "Ep 1 (Step 000005): Train loss 1.174, Val loss 1.103\n",
      "Ep 1 (Step 000010): Train loss 0.872, Val loss 0.944\n",
      "Ep 1 (Step 000015): Train loss 0.857, Val loss 0.906\n",
      "Ep 1 (Step 000020): Train loss 0.776, Val loss 0.881\n",
      "Ep 1 (Step 000025): Train loss 0.754, Val loss 0.859\n",
      "Ep 1 (Step 000030): Train loss 0.800, Val loss 0.836\n",
      "Ep 1 (Step 000035): Train loss 0.714, Val loss 0.809\n",
      "Ep 1 (Step 000040): Train loss 0.672, Val loss 0.806\n",
      "Ep 1 (Step 000045): Train loss 0.633, Val loss 0.789\n",
      "Ep 1 (Step 000050): Train loss 0.663, Val loss 0.783\n",
      "Ep 1 (Step 000055): Train loss 0.760, Val loss 0.763\n",
      "Ep 1 (Step 000060): Train loss 0.719, Val loss 0.743\n",
      "Ep 1 (Step 000065): Train loss 0.653, Val loss 0.735\n",
      "Ep 1 (Step 000070): Train loss 0.535, Val loss 0.732\n",
      "Ep 1 (Step 000075): Train loss 0.567, Val loss 0.736\n",
      "Ep 1 (Step 000080): Train loss 0.602, Val loss 0.731\n",
      "Ep 1 (Step 000085): Train loss 0.513, Val loss 0.715\n",
      "Ep 1 (Step 000090): Train loss 0.571, Val loss 0.696\n",
      "Ep 1 (Step 000095): Train loss 0.504, Val loss 0.687\n",
      "Ep 1 (Step 000100): Train loss 0.507, Val loss 0.682\n",
      "Ep 1 (Step 000105): Train loss 0.568, Val loss 0.674\n",
      "Ep 1 (Step 000110): Train loss 0.562, Val loss 0.669\n",
      "Ep 1 (Step 000115): Train loss 0.519, Val loss 0.665\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is prepared every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive:\n",
      "Ep 2 (Step 000120): Train loss 0.437, Val loss 0.670\n",
      "Ep 2 (Step 000125): Train loss 0.454, Val loss 0.686\n",
      "Ep 2 (Step 000130): Train loss 0.447, Val loss 0.681\n",
      "Ep 2 (Step 000135): Train loss 0.406, Val loss 0.677\n",
      "Ep 2 (Step 000140): Train loss 0.407, Val loss 0.676\n",
      "Ep 2 (Step 000145): Train loss 0.373, Val loss 0.677\n",
      "Ep 2 (Step 000150): Train loss 0.381, Val loss 0.674\n",
      "Ep 2 (Step 000155): Train loss 0.419, Val loss 0.676\n",
      "Ep 2 (Step 000160): Train loss 0.414, Val loss 0.686\n",
      "Ep 2 (Step 000165): Train loss 0.380, Val loss 0.688\n",
      "Ep 2 (Step 000170): Train loss 0.327, Val loss 0.679\n",
      "Ep 2 (Step 000175): Train loss 0.338, Val loss 0.668\n",
      "Ep 2 (Step 000180): Train loss 0.390, Val loss 0.657\n",
      "Ep 2 (Step 000185): Train loss 0.417, Val loss 0.659\n",
      "Ep 2 (Step 000190): Train loss 0.340, Val loss 0.650\n",
      "Ep 2 (Step 000195): Train loss 0.326, Val loss 0.635\n",
      "Ep 2 (Step 000200): Train loss 0.311, Val loss 0.632\n",
      "Ep 2 (Step 000205): Train loss 0.353, Val loss 0.628\n",
      "Ep 2 (Step 000210): Train loss 0.367, Val loss 0.628\n",
      "Ep 2 (Step 000215): Train loss 0.393, Val loss 0.635\n",
      "Ep 2 (Step 000220): Train loss 0.300, Val loss 0.647\n",
      "Ep 2 (Step 000225): Train loss 0.346, Val loss 0.662\n",
      "Ep 2 (Step 000230): Train loss 0.299, Val loss 0.657\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is cooked everyday by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: What is the capital of the United Kingdom\n",
      "Training completed in 7.04 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ise3wGjlB-iq",
   "metadata": {
    "id": "Ise3wGjlB-iq"
   },
   "source": [
    "- 从上面的输出结果可以看出，模型训练效果良好，这可以从不断下降的训练损失和验证损失值判断出来。\n",
    "- 此外，根据每个训练周期后打印的回复文本，我们可以看到模型能够正确遵循指令，将输入句子“厨师每天都烹饪这顿饭。”（The chef cooks the meal every day.）转换为 `被动语态` “这顿饭每天都被厨师烹饪。”（The meal is cooked every day by the chef.）（我们将在后面的章节中对回复进行正确格式化和评估）。\n",
    "- 最后，让我们来看看训练损失和验证损失曲线。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4acd368b-1403-4807-a218-9102e35bfdbb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "id": "4acd368b-1403-4807-a218-9102e35bfdbb",
    "outputId": "680da58a-9bd7-402d-ac95-470a4a29a6c4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from previous_chapters import plot_losses\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6777e0c4-d82c-46d8-84fb-1376c4f8bae0",
   "metadata": {
    "id": "6777e0c4-d82c-46d8-84fb-1376c4f8bae0"
   },
   "source": [
    "- 正如我们所见，在第一个训练周期开始时，损失急剧下降，这意味着模型快速开始学习。\n",
    "- 我们可以看到，在大约 1 个训练周期时开始出现轻微的过拟合现象。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b79a47-13f9-4d1f-87b1-3339bafaf2a3",
   "metadata": {
    "id": "87b79a47-13f9-4d1f-87b1-3339bafaf2a3"
   },
   "source": [
    "## 7.7 提取并保存回复"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a25cc88-1758-4dd0-b8bf-c044cbf2dd49",
   "metadata": {
    "id": "5a25cc88-1758-4dd0-b8bf-c044cbf2dd49"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/chapter-overview-6.webp?1\" width=800px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17510e9d-7727-4d58-ba9a-d82ec23c1427",
   "metadata": {
    "id": "17510e9d-7727-4d58-ba9a-d82ec23c1427"
   },
   "source": [
    "- 在本节中，我们保存测试集的回复，以便在下一节中进行评分。\n",
    "- 我们还保存一份模型的副本，以备将来使用。\n",
    "- 但首先，让我们简要查看一下微调（finetuned）模型生成的回复。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "VQ2NZMbfucAc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VQ2NZMbfucAc",
    "outputId": "8416b4ac-1993-4628-dea6-7789cdc8926c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using a simile.\n",
      "\n",
      "### Input:\n",
      "The car is very fast.\n",
      "\n",
      "Correct response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Model response:\n",
      ">> The car is as fast as a bullet.\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What type of cloud is typically associated with thunderstorms?\n",
      "\n",
      "Correct response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response:\n",
      ">> The type of cloud associated with thunderstorms is a cumulus cloud.\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Name the author of 'Pride and Prejudice'.\n",
      "\n",
      "Correct response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Model response:\n",
      ">> The author of 'Pride and Prejudice' is Jane Austen.\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "\n",
    "for entry in test_data[:3]:\n",
    "\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .strip()\n",
    ")\n",
    "\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
    "    print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ab64c1-586f-4939-8def-23feeb1b3599",
   "metadata": {
    "id": "49ab64c1-586f-4939-8def-23feeb1b3599"
   },
   "source": [
    "- 正如我们从测试集的指令、给定的回复以及模型生成的回复中可以看到的，该模型表现得相对较好。\n",
    "- 对于第一条和最后一条指令的答案显然是正确的。\n",
    "- 第二个答案很接近；模型回答的是“积云（cumulus cloud）”，而不是“积雨云（cumulonimbus）”（不过，请注意积云可以发展成积雨云，而积雨云能够产生雷暴）。\n",
    "- 最重要的是，我们可以看到，模型评估并不像前一章中那么简单直接，在前一章中我们只需计算正确的垃圾邮件/非垃圾邮件类别标签的百分比，就能得出分类准确率。\n",
    "- 在实践中，像聊天机器人这样经过指令微调的大语言模型（LLM）是通过多种方法进行评估的：\n",
    "  - 诸如MMLU（“大规模多任务语言理解评估”，[https://arxiv.org/abs/2009.03300](https://arxiv.org/abs/2009.03300)）这样的简答题和选择题基准测试，用于测试模型的知识水平。\n",
    "  - 与其他大语言模型（LLM）进行人类偏好比较，例如LMSYS聊天机器人竞技场（[https://arena.lmsys.org](https://arena.lmsys.org)）。\n",
    "  - 自动化对话基准测试，即使用另一个像GPT-4这样的大语言模型（LLM）来评估回复，例如AlpacaEval（[https://tatsu-lab.github.io/alpaca_eval/](https://tatsu-lab.github.io/alpaca_eval/)）。\n",
    "\n",
    "- 在下一节中，我们将使用一种类似于AlpacaEval的方法，并使用另一个大语言模型（LLM）来评估我们模型的回复；不过，我们将使用我们自己的测试集，而不是使用公开可用的基准数据集。\n",
    "- 为此，我们将模型的回复添加到 `test_data` 字典中，并将其保存为一个名为“instruction-data-with-response.json”的文件，以便记录留存，这样如果有需要，我们可以在不同的Python会话中加载并分析它。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "-PNGKzY4snKP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-PNGKzY4snKP",
    "outputId": "0453dfb3-51cd-49e2-9e63-f65b606c3478"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [02:33<00:00,  1.40s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
    "\n",
    "    test_data[i][\"model_response\"] = response_text\n",
    "\n",
    "\n",
    "with open(\"instruction-data-with-response.json\", \"w\") as file:\n",
    "    json.dump(test_data, file, indent=4)  # \"indent\" for pretty-printing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228d6fa7-d162-44c3-bef1-4013c027b155",
   "metadata": {
    "id": "228d6fa7-d162-44c3-bef1-4013c027b155"
   },
   "source": [
    "- 让我们再次检查其中一个条目，看看回复是否已正确添加到 `test_data` 字典中。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "u-AvCCMTnPSE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u-AvCCMTnPSE",
    "outputId": "ce3b2545-8990-4446-e44c-a945e0049c06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Rewrite the sentence using a simile.', 'input': 'The car is very fast.', 'output': 'The car is as fast as lightning.', 'model_response': 'The car is as fast as a bullet.'}\n"
     ]
    }
   ],
   "source": [
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b2f3f6-8569-405a-9db6-d47cba65608a",
   "metadata": {
    "id": "c1b2f3f6-8569-405a-9db6-d47cba65608a"
   },
   "source": [
    "- 最后，我们也保存模型，以防我们下一次想重新使用我们微调之后的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8cBU0iHmVfOI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8cBU0iHmVfOI",
    "outputId": "d6e7f226-9310-43f5-f31f-adc3a893a8e9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as gpt2-medium355M-sft.pth\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "file_name = f\"{re.sub(r'[ ()]', '', CHOOSE_MODEL) }-sft.pth\"\n",
    "torch.save(model.state_dict(), file_name)\n",
    "print(f\"Model saved as {file_name}\")\n",
    "\n",
    "# Load model via\n",
    "# model.load_state_dict(torch.load(\"gpt2-medium355M-sft.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obgoGI89dgPm",
   "metadata": {
    "id": "obgoGI89dgPm"
   },
   "source": [
    "## 7.8 评估经过微调的大语言模型（LLM）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805b9d30-7336-499f-abb5-4a21be3129f5",
   "metadata": {
    "id": "805b9d30-7336-499f-abb5-4a21be3129f5"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/chapter-overview-7.webp?1\" width=800px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d2b9d3-b6ff-4533-a89d-7b66079b4fd1",
   "metadata": {
    "id": "68d2b9d3-b6ff-4533-a89d-7b66079b4fd1"
   },
   "source": [
    "- 在本节中，我们使用另一个更大的大语言模型（LLM）来自动评估经过微调的大语言模型（LLM）的回复。\n",
    "- 具体来说，我们使用由Meta AI开发的一个经过指令微调的、拥有80亿参数的Llama 3模型，该模型可以通过ollama（[https://ollama.com](https://ollama.com)）在本地运行。\n",
    "- （或者，如果你更倾向于通过OpenAI API使用像GPT-4这样更强大的大语言模型（LLM），请查看[llm-instruction-eval-openai.ipynb](../03_model-evaluation/llm-instruction-eval-openai.ipynb)笔记本） "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea427a30-36ba-44e3-bb1f-eb0d7008d6e9",
   "metadata": {
    "id": "ea427a30-36ba-44e3-bb1f-eb0d7008d6e9"
   },
   "source": [
    "- Ollama是一款能够高效运行大语言模型（LLM）的应用程序。\n",
    "- 它是llama.cpp（[https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp)）的一个封装程序，llama.cpp 使用纯C/C++ 实现大语言模型（LLM）以最大限度地提高效率。\n",
    "- 请注意，它是一个用于使用大语言模型（LLM）生成文本（推理）的工具，而不是用于训练或微调大语言模型（LLM）的工具。\n",
    "- 在运行下面的代码之前，请访问[https://ollama.com](https://ollama.com) 并按照说明安装Ollama（例如，点击“下载”按钮，然后为你的操作系统下载Ollama应用程序）。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747a2fc7-282d-47ec-a987-ed0a23ed6822",
   "metadata": {
    "id": "747a2fc7-282d-47ec-a987-ed0a23ed6822"
   },
   "source": [
    "<!-- - For macOS and Windows users, click on the ollama application you downloaded; if it prompts you to install the command line usage, say \"yes\"\n",
    "- Linux users can use the installation command provided on the ollama website\n",
    "\n",
    "- In general, before we can use ollama from the command line, we have to either start the ollama application or run `ollama serve` in a separate terminal\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/ollama-run.webp?1\" width=700px>\n",
    "\n",
    "\n",
    "- With the ollama application or `ollama serve` running in a different terminal, on the command line, execute the following command to try out the 8-billion-parameter Llama 3 model (the model, which takes up 4.7 GB of storage space, will be automatically downloaded the first time you execute this command)\n",
    "\n",
    "```bash\n",
    "# 8B model\n",
    "ollama run llama3\n",
    "```\n",
    "\n",
    "\n",
    "The output looks like as follows\n",
    "\n",
    "```\n",
    "$ ollama run llama3\n",
    "pulling manifest\n",
    "pulling 6a0746a1ec1a... 100% ▕████████████████▏ 4.7 GB\n",
    "pulling 4fa551d4f938... 100% ▕████████████████▏  12 KB\n",
    "pulling 8ab4849b038c... 100% ▕████████████████▏  254 B\n",
    "pulling 577073ffcc6c... 100% ▕████████████████▏  110 B\n",
    "pulling 3f8eb4da87fa... 100% ▕████████████████▏  485 B\n",
    "verifying sha256 digest\n",
    "writing manifest\n",
    "removing any unused layers\n",
    "success\n",
    "```\n",
    "\n",
    "- Note that `llama3` refers to the instruction finetuned 8-billion-parameter Llama 3 model\n",
    "\n",
    "- Using ollama with the `\"llama3\"` model (a 8B parameter model) requires 16 GB of RAM; if this is not supported by your machine, you can try the smaller model, such as the 3.8B parameter phi-3 model by setting `model = \"phi-3\"`, which only requires 8 GB of RAM\n",
    "\n",
    "- Alternatively, you can also use the larger 70-billion-parameter Llama 3 model, if your machine supports it, by replacing `llama3` with `llama3:70b`\n",
    "\n",
    "- After the download has been completed, you will see a command line prompt that allows you to chat with the model\n",
    "\n",
    "- Try a prompt like \"What do llamas eat?\", which should return an output similar to the following\n",
    "\n",
    "```\n",
    ">>> What do llamas eat?\n",
    "Llamas are ruminant animals, which means they have a four-chambered\n",
    "stomach and eat plants that are high in fiber. In the wild, llamas\n",
    "typically feed on:\n",
    "1. Grasses: They love to graze on various types of grasses, including tall\n",
    "grasses, wheat, oats, and barley.\n",
    "``` -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e269d7",
   "metadata": {},
   "source": [
    "- 对于 macOS 和 Windows 用户，点击你下载的 Ollama 应用程序；如果它提示你安装命令行使用方式，请选择“是”。\n",
    "- Linux 用户可以使用 Ollama 网站上提供的安装命令。\n",
    "\n",
    "- 一般来说，在我们能够从命令行使用 Ollama 之前，我们必须启动 Ollama 应用程序，或者在一个单独的终端中运行 `ollama serve` 命令。\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/ollama-run.webp?1\" width=700px>\n",
    "\n",
    "- 在另一个终端中运行 Ollama 应用程序或 `ollama serve` 命令后，在命令行中执行以下命令，来试用拥有 80 亿参数的 Llama 3 模型（该模型占用 4.7GB 的存储空间，在你第一次执行此命令时，它将自动下载）。\n",
    "\n",
    "```bash\n",
    "# 80 亿参数模型\n",
    "ollama run llama3\n",
    "```\n",
    "\n",
    "输出内容如下：\n",
    "\n",
    "```\n",
    "$ ollama run llama3\n",
    "正在拉取清单\n",
    "正在拉取 6a0746a1ec1a... 100% ▕████████████████▏ 4.7 GB\n",
    "正在拉取 4fa551d4f938... 100% ▕████████████████▏  12 KB\n",
    "正在拉取 8ab4849b038c... 100% ▕████████████████▏  254 B\n",
    "正在拉取 577073ffcc6c... 100% ▕████████████████▏  110 B\n",
    "正在拉取 3f8eb4da87fa... 100% ▕████████████████▏  485 B\n",
    "正在验证 sha256 摘要\n",
    "正在写入清单\n",
    "正在移除所有未使用的层\n",
    "成功\n",
    "```\n",
    "\n",
    "- 请注意，`llama3` 指的是经过指令微调的、拥有 80 亿参数的 Llama 3 模型。\n",
    "\n",
    "- 使用带有 `\"llama3\"` 模型（一个 80 亿参数的模型）的 Ollama 需要 16GB 的内存；如果你的机器不支持，你可以尝试较小的模型，比如拥有 38 亿参数的 phi-3 模型，方法是设置 `model = \"phi-3\"`，该模型只需要 8GB 的内存。\n",
    "\n",
    "- 或者，如果你的机器支持的话，你也可以使用更大的、拥有 700 亿参数的 Llama 3 模型，只需将 `llama3` 替换为 `llama3:70b` 即可。\n",
    "\n",
    "- 下载完成后，你将看到一个命令行提示符，它允许你与该模型进行对话。\n",
    "\n",
    "- 尝试输入一个类似 “美洲驼吃什么？” 这样的提示，它应该会返回类似以下的输出：\n",
    "\n",
    "```\n",
    ">>> 美洲驼吃什么？\n",
    "美洲驼是反刍动物，这意味着它们有一个四室胃，并且食用高纤维的植物。在野外，美洲驼\n",
    "通常以以下食物为食：\n",
    "1. 草：它们喜欢啃食各种类型的草，包括高草、小麦、燕麦和大麦。\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7b341c-ba0e-40bb-a52c-cb328bbd1fe4",
   "metadata": {
    "id": "7b7b341c-ba0e-40bb-a52c-cb328bbd1fe4"
   },
   "source": [
    "- 你可以输入 `/bye` 来结束这次会话。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaf3e02-8ca0-4edf-be23-60625a5b14e3",
   "metadata": {
    "id": "faaf3e02-8ca0-4edf-be23-60625a5b14e3"
   },
   "source": [
    "- 以下代码会在继续使用Ollama来评估我们在上一节中生成的测试集回复之前，检查Ollama会话是否正在正确运行。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "026e8570-071e-48a2-aa38-64d7be35f288",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "026e8570-071e-48a2-aa38-64d7be35f288",
    "outputId": "e30d3533-e1f5-4aa9-b24f-33273fc7b30e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama running: True\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "def check_if_running(process_name):\n",
    "    running = False\n",
    "    for proc in psutil.process_iter([\"name\"]):\n",
    "        if process_name in proc.info[\"name\"]:\n",
    "            running = True\n",
    "            break\n",
    "    return running\n",
    "\n",
    "ollama_running = check_if_running(\"ollama\")\n",
    "\n",
    "if not ollama_running:\n",
    "    raise RuntimeError(\"Ollama not running. Launch ollama before proceeding.\")\n",
    "print(\"Ollama running:\", check_if_running(\"ollama\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "723c9b00-e3cd-4092-83c3-6e48b5cf65b0",
   "metadata": {
    "id": "723c9b00-e3cd-4092-83c3-6e48b5cf65b0"
   },
   "outputs": [],
   "source": [
    "# This cell is optional; it allows you to restart the notebook\n",
    "# and only run section 7.7 without rerunning any of the previous code\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "file_path = \"instruction-data-with-response.json\"\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    test_data = json.load(file)\n",
    "\n",
    "\n",
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3464705-d026-4594-977f-fb357e51c3a9",
   "metadata": {
    "id": "b3464705-d026-4594-977f-fb357e51c3a9"
   },
   "source": [
    "- 现在，除了我们之前使用的 `ollama run` 命令来与模型进行交互之外，还有一种替代方法，即通过Python利用其REST API ，可通过以下函数来实现。\n",
    "- 在运行此笔记本中的下一个代码单元之前，请确保Ollama仍在运行（之前的代码单元应该会打印出 `Ollama running: True`）。\n",
    "- 接下来，运行以下代码单元来查询模型。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e3ae0e10-2b28-42ce-8ea2-d9366a58088f",
   "metadata": {
    "id": "e3ae0e10-2b28-42ce-8ea2-d9366a58088f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llamas are herbivores, which means they primarily eat plants and plant-based foods. Their diet typically consists of:\n",
      "\n",
      "1. Grasses: Llamas love to graze on various types of grasses, including tall grasses, short grasses, and grassy weeds.\n",
      "2. Hay: High-quality hay, such as timothy hay or alfalfa hay, is a staple in a llama's diet. It provides essential nutrients like fiber, protein, and vitamins.\n",
      "3. Grains: Llamas may also be fed grains like oats, barley, or corn, but these should not make up more than 10% of their diet.\n",
      "4. Fruits and vegetables: Fresh fruits and vegetables, such as apples, carrots, and sweet potatoes, can be given to llamas as treats or added to their hay.\n",
      "5. Browse: Llamas may also eat browse, which includes leaves, twigs, and other vegetation from trees and shrubs.\n",
      "\n",
      "It's essential to note that llamas have a unique digestive system, with a four-chambered stomach, which allows them to break down and extract nutrients from plant material more efficiently than many other animals. However, this also means they can be prone to certain health issues if their diet is not balanced or if they eat too much of the wrong foods.\n",
      "\n",
      "A good rule of thumb for llama owners is to provide a high-quality hay-based diet with limited amounts of grains and treats, and to ensure access to fresh water at all times.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "def query_model(\n",
    "    prompt,\n",
    "    model=\"llama3\",\n",
    "    url=\"http://localhost:11434/api/chat\"\n",
    "):\n",
    "    # Create the data payload as a dictionary\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"options\": {     # Settings below are required for deterministic responses\n",
    "            \"seed\": 123,\n",
    "            \"temperature\": 0,\n",
    "            \"num_ctx\": 2048\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    # Convert the dictionary to a JSON formatted string and encode it to bytes\n",
    "    payload = json.dumps(data).encode(\"utf-8\")\n",
    "\n",
    "    # Create a request object, setting the method to POST and adding necessary headers\n",
    "    request = urllib.request.Request(\n",
    "        url,\n",
    "        data=payload,\n",
    "        method=\"POST\"\n",
    "    )\n",
    "    request.add_header(\"Content-Type\", \"application/json\")\n",
    "\n",
    "    # Send the request and capture the response\n",
    "    response_data = \"\"\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        # Read and decode the response\n",
    "        while True:\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            if not line:\n",
    "                break\n",
    "            response_json = json.loads(line)\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "\n",
    "    return response_data\n",
    "\n",
    "\n",
    "model = \"llama3.2\"\n",
    "result = query_model(\"What do Llamas eat?\", model)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207ae28f-0f8c-4fda-aeef-e7e3046249cc",
   "metadata": {
    "id": "207ae28f-0f8c-4fda-aeef-e7e3046249cc"
   },
   "source": [
    "- 现在，利用我们上面定义的 `query_model` 函数，我们可以评估经过微调的模型的回复；让我们对前一节中查看过的前 3 条测试集回复进行尝试。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "86b839d4-064d-4178-b2d7-01691b452e5e",
   "metadata": {
    "id": "86b839d4-064d-4178-b2d7-01691b452e5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Model response:\n",
      ">> The car is as fast as a bullet.\n",
      "\n",
      "Score:\n",
      ">> To rewrite the sentence using a simile, we need to compare the speed of the car to something else.\n",
      "\n",
      "Correct output: The car is as fast as lightning.\n",
      "\n",
      "Score: 100\n",
      "\n",
      "The model response \"The car is as fast as a bullet\" is close, but not perfect. A simile should use \"like\" or \"as\" to make the comparison, whereas \"as fast as a bullet\" implies that the car is literally a bullet, which isn't the intended meaning.\n",
      "\n",
      "A better score for the model response would be around 80-90, as it's close to the correct form but not quite there.\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response:\n",
      ">> The type of cloud associated with thunderstorms is a cumulus cloud.\n",
      "\n",
      "Score:\n",
      ">> I would rate the model response a 20.\n",
      "\n",
      "The reason for this low score is that the model response contains an error in its classification of clouds. Cumulonimbus clouds are indeed associated with thunderstorms, but cumulus clouds are typically associated with fair weather and are often seen on warm, sunny days. The correct term should be \"cumulonimbus\" instead of \"cumulus\".\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Model response:\n",
      ">> The author of 'Pride and Prejudice' is Jane Austen.\n",
      "\n",
      "Score:\n",
      ">> ### Input\n",
      "Name the author of 'Pride and Prejudice'.\n",
      "\n",
      "### Output\n",
      "Jane Austen.\n",
      "\n",
      "### Score: 100/100\n",
      "\n",
      "The response is correct because it directly answers the question by providing the name of the author of \"Pride and Prejudice\". The sentence structure is simple and clear, making it easy to understand. There are no grammatical errors or ambiguities in the response.\n",
      "\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "for entry in test_data[:3]:\n",
    "    prompt = (\n",
    "        f\"Given the input `{format_input(entry)}` \"\n",
    "        f\"and correct output `{entry['output']}`, \"\n",
    "        f\"score the model response `{entry['model_response']}`\"\n",
    "        f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "    )\n",
    "    print(\"\\nDataset response:\")\n",
    "    print(\">>\", entry['output'])\n",
    "    print(\"\\nModel response:\")\n",
    "    print(\">>\", entry[\"model_response\"])\n",
    "    print(\"\\nScore:\")\n",
    "    print(\">>\", query_model(prompt, \"llama3.2\"))    # 我本地默认安装 llama3.2\n",
    "    print(\"\\n-------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b114fd65-9cfb-45f6-ab74-8331da136bf3",
   "metadata": {
    "id": "b114fd65-9cfb-45f6-ab74-8331da136bf3"
   },
   "source": [
    "- 正如我们所见，Llama 3 模型给出了合理的评估，并且当模型的回答不完全正确时，也会给出部分分数，就像我们从“积云（cumulus cloud）”这个答案中看到的那样。\n",
    "- 请注意，之前的提示会返回非常详细的评估内容；我们可以调整提示内容，使其生成 0 到 100 之间的整数评分（100 分表示最佳），以便为我们的模型计算平均得分。\n",
    "- 在配备 M3 芯片的 MacBook Air 笔记本电脑上，对测试集中的 110 个条目进行评估大约需要 1 分钟。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9d7bca69-97c4-47a5-9aa0-32f116fa37eb",
   "metadata": {
    "id": "9d7bca69-97c4-47a5-9aa0-32f116fa37eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  22%|██▏       | 24/110 [00:07<00:33,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: Fish: Shark, Trout\n",
      "Mammals: Dolphin\n",
      "\n",
      "90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  71%|███████   | 78/110 [00:25<00:17,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: I would classify the given sentence into two categories:\n",
      "\n",
      "1. Declarative: 80\n",
      "2. Interrogative: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  95%|█████████▍| 104/110 [00:33<00:02,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: Hasta luego\n",
      "\n",
      "Score: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  98%|█████████▊| 108/110 [00:35<00:01,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: The classification of the sentence 'Please open the door.' is imperative.\n",
      "Score: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries: 100%|██████████| 110/110 [00:36<00:00,  3.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scores: 106 of 110\n",
      "Average score: 55.99\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_model_scores(json_data, json_key, model=\"llama3\"):\n",
    "    scores = []\n",
    "    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n",
    "        prompt = (\n",
    "            f\"Given the input `{format_input(entry)}` \"\n",
    "            f\"and correct output `{entry['output']}`, \"\n",
    "            f\"score the model response `{entry[json_key]}`\"\n",
    "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "            f\"Respond with the integer number only. You should not give any explaination on score. Only output an integer number\"\n",
    "        )\n",
    "        score = query_model(prompt, model)\n",
    "        try:\n",
    "            scores.append(int(score))\n",
    "        except ValueError:\n",
    "            print(f\"Could not convert score: {score}\")\n",
    "            continue\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "scores = generate_model_scores(test_data, \"model_response\", model=\"llama3.2\")\n",
    "print(f\"Number of scores: {len(scores)} of {len(test_data)}\")\n",
    "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407f08d5-9ada-4301-9ebc-f0533c76d3f2",
   "metadata": {
    "id": "407f08d5-9ada-4301-9ebc-f0533c76d3f2"
   },
   "source": [
    "- 我们的模型获得了超过 50 分的平均得分，我们可以将其作为参考点，用于将该模型与其他模型进行比较，或者尝试其他可能改进模型的训练设置。\n",
    "- 请注意，截至撰写本文时，Ollama 在不同操作系统上并非完全具有确定性，因此你得到的分数可能会与上面显示的分数略有不同。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6408768b-2784-44f1-b48e-aed0c1eb9b94",
   "metadata": {
    "id": "6408768b-2784-44f1-b48e-aed0c1eb9b94"
   },
   "source": [
    "- 作为参考，原始的：\n",
    "  - Llama 3 80B 基础模型的得分为 58.51。\n",
    "  - Llama 3 80B 指令模型的得分为 82.65。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412d7325-284a-446c-92a1-5aa8acc52dee",
   "metadata": {
    "id": "412d7325-284a-446c-92a1-5aa8acc52dee"
   },
   "source": [
    "## 7.9 Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbfdc6b",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>英文原文</summary>\n",
    "\n",
    "  ### 7.9.1 What's next\n",
    "\n",
    "- This marks the final chapter of this book\n",
    "- We covered the major steps of the LLM development cycle: implementing an LLM architecture, pretraining an LLM, and finetuning it\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/final-overview.webp?1\" width=500px>\n",
    "\n",
    "- An optional step that is sometimes followed after instruction finetuning, as described in this chapter, is preference finetuning\n",
    "- Preference finetuning process can be particularly useful for customizing a model to better align with specific user preferences; see the [../04_preference-tuning-with-dpo](../04_preference-tuning-with-dpo) folder if you are interested in this\n",
    "\n",
    "- This GitHub repository also contains a large selection of additional bonus material you may enjoy; for more information, please see the [Bonus Material](https://github.com/rasbt/LLMs-from-scratch?tab=readme-ov-file#bonus-material) section on this repository's README page\n",
    "\n",
    "### 7.9.2 Staying up to date in a fast-moving field\n",
    "\n",
    "- No code in this section\n",
    "\n",
    "### 7.9.3 Final words\n",
    "\n",
    "- I hope you enjoyed this journey of implementing an LLM from the ground up and coding the pretraining and finetuning functions\n",
    "- In my opinion, implementing an LLM from scratch is the best way to understand how LLMs work; I hope you gained a better understanding through this approach\n",
    "- While this book serves educational purposes, you may be interested in using different and more powerful LLMs for real-world applications\n",
    "  - For this, you may consider popular tools such as axolotl ([https://github.com/OpenAccess-AI-Collective/axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)) or LitGPT ([https://github.com/Lightning-AI/litgpt](https://github.com/Lightning-AI/litgpt)), which I help developing\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d10bbe",
   "metadata": {},
   "source": [
    "### 7.9.1 接下来的计划\n",
    "\n",
    "- 这标志着本书的最后一章。\n",
    "- 我们涵盖了大语言模型（LLM）开发周期的主要步骤：实现一个大语言模型架构、预训练一个大语言模型，以及对其进行微调。\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/final-overview.webp?1\" width=1000px>\n",
    "\n",
    "- 正如本章所描述的，在指令微调之后，有时会进行的一个可选步骤是偏好微调。\n",
    "- 偏好微调过程对于定制模型使其更好地符合特定用户偏好特别有用；如果你对此感兴趣，可以查看 [../04_preference-tuning-with-dpo](../04_preference-tuning-with-dpo) 文件夹。\n",
    "\n",
    "- 这个 GitHub 存储库还包含大量你可能会感兴趣的额外补充材料；更多信息，请查看该存储库 README 页面上的 [补充材料](https://github.com/rasbt/LLMs-from-scratch?tab=readme-ov-file#bonus-material) 部分。\n",
    "\n",
    "### 7.9.2 在快速发展的领域中保持与时俱进\n",
    "\n",
    "- 本节无代码内容。\n",
    "\n",
    "### 7.9.3 结束语\n",
    "\n",
    "- 我希望你享受了这次从头开始实现一个大语言模型（LLM），并为预训练和微调函数编写代码的旅程。\n",
    "- 在我看来，从头开始实现一个大语言模型是理解大语言模型如何工作的最佳方式；我希望通过这种方式你能有更深入的理解。\n",
    "- 虽然本书具有教育意义，但你可能有兴趣在实际应用中使用不同且更强大的大语言模型。\n",
    "  - 为此，你可以考虑一些流行的工具，如 axolotl（[https://github.com/OpenAccess-AI-Collective/axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)）或 LitGPT（[https://github.com/Lightning-AI/litgpt](https://github.com/Lightning-AI/litgpt)），我也参与了这些工具的开发工作。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9853e7f-a81a-4806-9728-be1690807185",
   "metadata": {
    "id": "f9853e7f-a81a-4806-9728-be1690807185"
   },
   "source": [
    "<details>\n",
    "  <summary>Original</summary>\n",
    "\n",
    "  ## Summary and takeaways\n",
    "\n",
    "- See the [./gpt_instruction_finetuning.py](./gpt_instruction_finetuning.py) script, a self-contained script for classification finetuning\n",
    "- [./ollama_evaluate.py](./ollama_evaluate.py) is a standalone script based on section 7.8 that evaluates a JSON file containing \"output\" and \"response\" keys via Ollama and Llama 3\n",
    "- The [./load-finetuned-model.ipynb](./load-finetuned-model.ipynb) notebook illustrates how to load the finetuned model in a new session\n",
    "- You can find the exercise solutions in [./exercise-solutions.ipynb](./exercise-solutions.ipynb)\n",
    "\n",
    "</details>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb11476",
   "metadata": {},
   "source": [
    "## 总结与要点\n",
    "\n",
    "- 请查看 [./gpt_instruction_finetuning.py](./gpt_instruction_finetuning.py) 脚本，这是一个用于分类微调的独立脚本。\n",
    "- [./ollama_evaluate.py](./ollama_evaluate.py) 是一个基于 7.8 节的独立脚本，它通过 Ollama 和 Llama 3 对一个包含 “output” 和 “response” 键的 JSON 文件进行评估。\n",
    "- [./load-finetuned-model.ipynb](./load-finetuned-model.ipynb) 笔记本展示了如何在新的会话中加载经过微调的模型。\n",
    "- 你可以在 [./exercise-solutions.ipynb](./exercise-solutions.ipynb) 中找到练习题的答案。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cc51ec-e06c-4470-b626-48401a037851",
   "metadata": {},
   "source": [
    "## 接下来做什么？\n",
    "\n",
    "- 恭喜你读完了这本书；如果你还在寻找更多的学习资源，我在这个 GitHub 仓库中添加了几个补充章节，你可能会感兴趣。\n",
    "- 完整的补充材料列表可以在主 README 文件的 [补充材料](https://github.com/rasbt/LLMs-from-scratch?tab=readme-ov-file#bonus-material) 部分查看。\n",
    "- 在这里重点介绍一些我特别推荐的内容：\n",
    "  1. [用于大语言模型校准的直接偏好优化（从头开始）](../04_preference-tuning-with-dpo/dpo-from-scratch.ipynb) 实现了一种流行的偏好微调机制，能让本章中的模型更符合人类偏好。\n",
    "  2. [从头开始实现 Llama 3.2（独立笔记本）](../../ch05/07_gpt_to_llama/standalone-llama32.ipynb)，这是对 Meta AI 广受欢迎的 Llama 3.2 的从头开始实现，包括加载官方预训练权重；如果你还想做一些额外的实验，可以用 `Llama3Model` 类替换每一章中的 `GPTModel` 模型（应该可以直接一对一替换）。\n",
    "  3. [将 GPT 转换为 Llama](../../ch05/07_gpt_to_llama) 包含逐步指导的代码，解释了 GPT-2 与各种 Llama 模型之间的区别。\n",
    "  4. [理解嵌入层和线性层之间的区别](../../ch02/03_bonus_embedding-vs-matmul/embeddings-and-linear-layers.ipynb) 是一个概念性的解释，说明了我们在大语言模型输入阶段使用的 PyTorch 中的 `Embedding` 层，在数学上等同于应用于独热编码数据的线性层。\n",
    "- 祝你继续阅读愉快！ "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
